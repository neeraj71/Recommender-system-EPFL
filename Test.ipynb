{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains test results on three Baseline, MF-ALS, MF-SGD, NNMF and KNN: user-based and item based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from helpers import load_data\n",
    "from utils import split_data\n",
    "\n",
    "from baselines import *\n",
    "from matrix_factorization import matrix_factorization_sgd, write_sgd_prediction, matrix_factorization_als, write_als_prediction\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process data and traform to sparse matrix\n",
    "path_dataset = \"./data/data_train.csv\"\n",
    "ratings = load_data(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of original ratings : (10000, 1000)\n",
      "Shape of valid ratings (and of train and test data) : (9990, 999)\n",
      "Total number of nonzero elements in original data : 1176952\n",
      "Total number of nonzero elements in train data : 1068523\n",
      "Total number of nonzero elements in test data : 108350\n"
     ]
    }
   ],
   "source": [
    "#load data and store in pickle \n",
    "_, train, test = split_data(ratings, 10, verbose=True)\n",
    "with open('./data/pickle/train.pickle', 'wb') as file:\n",
    "    pickle.dump(train, file)\n",
    "with open('./data/pickle/test.pickle', 'wb') as file:\n",
    "    pickle.dump(test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00e+00, 8.40e+01, 4.00e+00],\n",
       "       [1.00e+00, 4.39e+02, 4.00e+00],\n",
       "       [2.00e+00, 4.80e+01, 4.00e+00],\n",
       "       ...,\n",
       "       [9.99e+03, 7.36e+02, 3.00e+00],\n",
       "       [9.99e+03, 9.06e+02, 4.00e+00],\n",
       "       [9.99e+03, 9.85e+02, 1.00e+00]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function change a sparse matrix to a n*3 array\n",
    "# It makes our blending part easy to implement  \n",
    "def toarray(matrix):\n",
    "    nnz_row, nnz_col = matrix.nonzero()\n",
    "    rat = matrix[matrix.nonzero()].toarray().reshape((len(nnz_col),1))\n",
    "    nnz_row += 1\n",
    "    nnz_col += 1\n",
    "    return np.concatenate((nnz_row.reshape(len(nnz_col),1), nnz_col.reshape(len(nnz_col),1), rat), axis=1)\n",
    "\n",
    "test_matrix = toarray(test)\n",
    "test_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrameramepandas as pd\n",
    "def savepred(a, name):\n",
    "    a = pd.DataFrame(a)\n",
    "    a.to_csv(\"./data/name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we are going to use global mean, user mean and item mean to test the error of baseline model. It is reasonable that these model do not have good performace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Global Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 25.113199949264526 seconds ---\n",
      "Global mean RMSE : 1.1183506557779523\n"
     ]
    }
   ],
   "source": [
    "#to test the result of global mean\n",
    "start_time = time.time()\n",
    "global_mean_rmse = global_mean_test(ratings, min_num_ratings=10)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('Global mean RMSE : {}'.format(global_mean_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computer a test prediction for blending\n",
    "blend_GlbMean = np.ones((test_matrix.shape[0], 1)) * global_mean(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 User Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 166.68256092071533 seconds ---\n",
      "User mean RMSE : 1.0289888944873853\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "user_mean_rmse = user_mean_test(ratings, min_num_ratings=10)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('User mean RMSE : {}'.format(user_mean_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computer a test prediction for blending\n",
    "blend_UserMean = compute_user_means(test)\n",
    "# blend_UserMean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Item Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 31.795614004135132 seconds ---\n",
      "Item mean RMSE : 1.0938352842783858\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "item_mean_rmse = item_mean_test(ratings, min_num_ratings=10)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('Item mean RMSE : {}'.format(item_mean_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computer a test prediction for blending\n",
    "blend_ItemMean = compute_item_means(test)\n",
    "# blend_ItemMean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Matrix Facrization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to tune the hyper-parameters for matrix factorisation with SGD, a k-fold cross validation is used, with $k$ set to be 5. By using grid search, the item penalisation coefficient $\\lambda_{it} = 0.25$, the user penalisation coefficient $\\lambda_{us} = 0.01$, the latent variable $k=20$. This process of SGD is iterated by 50 times, when the change between iterations are small enough to be ignored. The running time of this method is 2067s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning the matrix factorization using SGD...\n",
      "Final RMSE on train data: 0.9912955138845942\n",
      "Final RMSE on test data: 1.0001463113122229.\n",
      "--- 2067.5607390403748 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9912955138845942, 1.0001463113122229)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_rmse, test_rmse, _, _ = matrix_factorization_sgd(train, test, gamma=0.012, verbose=True, \n",
    "                                                       lambda_user=0.01, lambda_item=0.25,\n",
    "                                                       num_epochs=50, num_features=20)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "train_rmse, test_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same idea as SGD, matrix factorisation with ALS is tuned by using grid search as well. After setting $k=20$, which is found at the initial cursory grid search, it is observed that most results of ALS are better than SGD. Hence, its parameter optimisation is investigated more precisely with a finer grid in grid search. below shows the grid search plot, where the brightest area indicates the most precise prediction. The best-tuned model found turns out to have the item penalisation coefficient $\\lambda_{it} = 0.575$, the user penalisation coefficient $\\lambda_{us} = 0.014$. The model is trained such that the change of improvement between each iteration is neglectable ($10^{-6}$). The running time of this method is 1847s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning the matrix factorization using ALS...\n",
      "Final RMSE on train data: 0.9082974241119364\n",
      "Final RMSE on test data: 0.983983639\n",
      "--- 1847.3403561115265 seconds ---\n",
      "0.9082974241119364 0.983983639\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_rmse, test_rmse, _, _ = matrix_factorization_als(train, test, verbose=True, stop_criterion=0.00001,\n",
    "                                                       lambda_user=0.14, lambda_item=0.575, num_features=20)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(train_rmse, test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. NNMF section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Neural network matrix factorization` or NNMF, for shortâ€”dominates standard low-rank techniques on a suite of benchmark but is dominated by some recent proposals that take advantage of the graph features. Given the vast range of architectures, activation functions, regularizers, and optimizationtechniques that could be used within the NNMF framework, it seems likely the true potential of the approach has yet to be reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is presented by Gintare Karolina Dziugaite.The neural network contains three layers with 50 units. \n",
    "After tuning hyper-parameter, we set lamda=1.4841, D=40, D_prim=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import nnmf.nnmf \n",
    "import nnmf.predict\n",
    "import nnmf.split_data  \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in data\n",
      "number of items: 10000, number of users: 1000\n",
      "Data subsets:\n",
      "Train: 953330\n",
      "Validation: 105926\n",
      "Test: 117696\n"
     ]
    }
   ],
   "source": [
    "#split train and test set by our defaut setting\n",
    "nnmf.split_data.split_nnmf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network & initializing variables\n",
      "Reading in data\n",
      "[start] Train error: 95769.312500, Train RMSE: 1.823475; Valid RMSE: 1.820590\n",
      "Early stopping (0.9900772571563721 vs. 0.9904701709747314)...\n",
      "Loading best checkpointed model\n",
      "./model/nnmf.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./model/nnmf.ckpt\n",
      "Final train RMSE: 0.958625078201294\n",
      "Final test RMSE: 0.9892654418945312\n"
     ]
    }
   ],
   "source": [
    "#training nnmf (test_ratio=0.1)\n",
    "#if you want to see the process, set verbose=True\n",
    "nnmf.nnmf.do_nnmf(mode='train', verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN is a prediction algorithm that computes a prediction for the rating exploiting a weighted sum of the other users/items ratings with the help of a similarity metric, in our case Pearson Baseline. This algorithm implemented in the Python Surprise library. min_k (int) â€“ The minimum number of neighbors to take into account for aggregation. If there are not enough neighbors, the prediction is set the the global mean of all ratings. Default is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import BaselineOnly\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import KNNBasic\n",
    "import os\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read and set train and test in surprise library\n",
    "file_path_train = os.path.expanduser('./data/mov_kaggle.all')\n",
    "reader = Reader(line_format='user item rating', sep='\\t')\n",
    "data = Dataset.load_from_file(file_path_train, reader=reader)\n",
    "trainset, testset = train_test_split(data, test_size=.1)   #test ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## user_based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.019656231639403"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_options = {'name': 'msd',\n",
    "               'user_based': True  # compute  similarities between items\n",
    "               }\n",
    "algo = KNNBasic(k = 80, sim_options=sim_options)\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## item_based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0234965094325155"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_options = {'name': 'msd',\n",
    "               'user_based': False  # compute  similarities between users\n",
    "               }\n",
    "algo = KNNBasic(k = 20, sim_options=sim_options)\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blend import *\n",
    "def als_pred(user_features, item_features, input_path,\n",
    "                          verbose=False):\n",
    "    nnz_train = np.array(pd.read_csv(input_path, sep='\\t', header=None,))\n",
    "    als = []\n",
    "    for i, k in enumerate(nnz_train):\n",
    "        item = int(k[0])-1\n",
    "        user = int(k[1])-1\n",
    "        als.append( user_features[:, user].T.dot(item_features[:, item]))\n",
    "    return als"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n",
      "Learning the matrix factorization using ALS...\n",
      "--- 723.7915439605713 seconds ---\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../data/mov_kaggle.all' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-6b4ed7d9bbd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m                                                               lambda_user=0.04, lambda_item=0.2, num_features=20)\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- %s seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mre_als\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mals_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../data/mov_kaggle.all'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/mov_kaggle.train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrm\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mblend_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre_als\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre_als\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre_nnmf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre_als\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-740bf61045a6>\u001b[0m in \u001b[0;36mals_pred\u001b[0;34m(user_features, item_features, input_path, verbose)\u001b[0m\n\u001b[1;32m      2\u001b[0m def als_pred(user_features, item_features, input_path,\n\u001b[1;32m      3\u001b[0m                           verbose=False):\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnnz_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnz_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhenchensu/anaconda2/envs/ada/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhenchensu/anaconda2/envs/ada/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhenchensu/anaconda2/envs/ada/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhenchensu/anaconda2/envs/ada/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhenchensu/anaconda2/envs/ada/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../data/mov_kaggle.all' does not exist"
     ]
    }
   ],
   "source": [
    "import nnmf.split_data\n",
    "als_predictions = nnmf.split_data.load_data2('./data/predictions/als_prediction.csv')\n",
    "\n",
    "# Train the blending with Ridge Regression\n",
    "# print('Training the blending with ridge regression')\n",
    "alpha = 0.1\n",
    "als_predictions\n",
    "start_time = time.time()\n",
    "_, user_features, item_features = matrix_factorization_als(ratings, None, verbose=True, stop_criterion=0.00001, \n",
    "                                                              lambda_user=0.04, lambda_item=0.2, num_features=20)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "re_als = als_pred(user_features, item_features, input_path='../data/mov_kaggle.all' )\n",
    "label = pd.read_csv('../data/mov_kaggle.all', sep='\\t', header=None).iloc[:,2]\n",
    "w, rm= blend_train(als = np.array(re_als).reshape((len(re_als),1)), alpha=0.1, label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (<ipython-input-100-5ebc6bb01efb>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-100-5ebc6bb01efb>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def blend_train(als=None, sgd=None, user__mean=None, knn_user=None, knn_item=None, svd=None, svdpp=None, nnmf=None, alpha=None, label):\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "def blend_train(als=None, sgd=None, user__mean=None, knn_user=None, knn_item=None, svd=None, svdpp=None, nnmf=None, alpha=None, label=None):\n",
    "    \"\"\"\n",
    "    This function train blending model for our prediction of algorithms \n",
    "    :param als: is prediction array of als algrithm with shape of n*1 \n",
    "    :param sgd: is prediction array of sgd algrithm with shape of n*1 \n",
    "    :param user__mean: is prediction array of user__mean algrithm with shape of n*1 \n",
    "    :param knn_user is prediction array of knn_user algrithm with shape of n*1 \n",
    "    :param knn_item is prediction array of knn_item algrithm with shape of n*1 \n",
    "    :param svd is prediction array of svd algrithm with shape of n*1 \n",
    "    :param svdpp is prediction array of svd algrithm with shape of n*1 \n",
    "    :param nnmf: is prediction array of nnmf algrithm with shape of n*1 \n",
    "    :param alpha: the parameter for regualrize\n",
    "    :param label: the true raking \n",
    "    :return: weight w and RMSE of blending\n",
    "    \"\"\"\n",
    "\n",
    "    m_train = np.concatenate((als, sgd, user__mean, knn_user, knn_item, svd, svdpp, nnmf), axis=1)\n",
    "    y_train = lable\n",
    "    # Ridge Regression\n",
    "    w = np.linalg.solve(m_train.T.dot(m_train) + alpha * np.eye(m_train.shape[1]), m_train.T.dot(y_train))\n",
    "    \n",
    "    y_predict_train = m_train.dot(w)\n",
    "    # Cut predictions that are too high and too low\n",
    "    for i in range(len(y_predict_train)):\n",
    "        y_predict_train[i] = min(5, np.round(y_predict_train[i]))\n",
    "        y_predict_train[i] = max(1, np.round(y_predict_train[i]))\n",
    "\n",
    "    return w, np.sqrt(np.mean((y_train - y_predict_train) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-a29ecdca1053>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/mov_kaggle.all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrm\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mblend_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre_als\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre_als\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/zhenchensu/2018-ML/ML_course_2/ML_course/projects/project2/submit/blend.py\u001b[0m in \u001b[0;36mblend_train\u001b[0;34m(als, sgd, user__mean, knn_user, knn_item, svd, svdpp, nnmf, alpha, label)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mm_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser__mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknn_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknn_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvdpp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnmf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Ridge Regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "\n",
    "label = pd.read_csv('./data/mov_kaggle.all', sep='\\t', header=None).iloc[:,2]\n",
    "w, rm= blend_train(als = np.array(re_als).reshape((len(re_als),1)), alpha=0.1, label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1176952,)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1006293, 1)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(re_als).reshape((len(re_als),1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
